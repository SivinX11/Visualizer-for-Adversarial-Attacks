## Team:Fluorine
# Adversarial Attack & Classifications Vizualizer.
<img src="https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png" height="18" >   <img src="https://raw.githubusercontent.com/TanmayKhot/Fluorine/adv/Images/advtool.JPG" height="24">    <img src="https://raw.githubusercontent.com/TanmayKhot/Fluorine/adv/Images/streamlit.png" height="18"> 
<br>
It is a web-based application to analyze the state of the art models based on their susceptibility against various adversarial attacks and also to find a model perfect for user's needs according to the dataset. 
<br>

# Features
- Visualize the classification task by the various state of the art models with your own images.
- The input images can be either uploaded from local storage or image link.
- Test out various adversarial attacks on your image and check the misclassification.
- Visualize, compare and contrast the classification on the image before and after the attack.
- Would help one to analyze which model is better for their personal use.
